{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vivit_metrics = pd.read_csv(\"vivit_validation_metrics_all_runs.csv\")\n",
    "\n",
    "precision_metrics = vivit_metrics[vivit_metrics[\"Metric\"] == \"Precision\"]\n",
    "recall_metrics = vivit_metrics[vivit_metrics[\"Metric\"] == \"Recall\"]\n",
    "accuracy_metrics = vivit_metrics[vivit_metrics[\"Metric\"] == \"Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normality Tests (Shapiro-Wilk):\n",
      "----------------------------------------\n",
      "Accuracy   PASSED   (p=0.2812)\n",
      "Recall     FAILED   (p=0.0401)\n",
      "          WARNING: Recall is not normally distributed!\n",
      "Precision  PASSED   (p=0.1637)\n",
      "\n",
      "Equality of Variance Tests (Levene):\n",
      "----------------------------------------\n",
      "\n",
      "Equal Variances Test: PASSED (p=0.7629)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro, levene\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def interpret_pvalue(pvalue: float, threshold: float = 0.05) -> str:\n",
    "    \"\"\"Interpret p-value against significance threshold\"\"\"\n",
    "    return \"PASSED\" if pvalue > threshold else \"FAILED\"\n",
    "\n",
    "\n",
    "def check_normality(metric_name: str, values: np.ndarray) -> None:\n",
    "    \"\"\"Test and interpret normality for a given metric\"\"\"\n",
    "    statistic, pvalue = shapiro(values)\n",
    "    result = interpret_pvalue(pvalue)\n",
    "    print(f\"{metric_name:10} {result:8} (p={pvalue:.4f})\")\n",
    "    if result == \"FAILED\":\n",
    "        print(f\"          WARNING: {metric_name} is not normally distributed!\")\n",
    "\n",
    "\n",
    "def check_equal_variances(\n",
    "    accuracy: np.ndarray,\n",
    "    recall: np.ndarray,\n",
    "    precision: np.ndarray,\n",
    ") -> None:\n",
    "    \"\"\"Test and interpret equality of variances across metrics\"\"\"\n",
    "    statistic, pvalue = levene(accuracy, recall, precision)\n",
    "    result = interpret_pvalue(pvalue)\n",
    "    print(f\"\\nEqual Variances Test: {result} (p={pvalue:.4f})\")\n",
    "    if result == \"FAILED\":\n",
    "        print(\"WARNING: Metrics have significantly different variances!\")\n",
    "\n",
    "\n",
    "accuracy_values = accuracy_metrics[\"Value\"]\n",
    "recall_values = recall_metrics[\"Value\"]\n",
    "precision_values = precision_metrics[\"Value\"]\n",
    "\n",
    "print(\"\\nNormality Tests (Shapiro-Wilk):\")\n",
    "print(\"-\" * 40)\n",
    "check_normality(\"Accuracy\", accuracy_values)\n",
    "check_normality(\"Recall\", recall_values)\n",
    "check_normality(\"Precision\", precision_values)\n",
    "\n",
    "print(\"\\nEquality of Variance Tests (Levene):\")\n",
    "print(\"-\" * 40)\n",
    "# todo: here we should check against the values of convnext,\n",
    "# for now I'm leaving this as it is to test it is working\n",
    "check_equal_variances(accuracy_values, recall_values, precision_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
